"""
Test the improved AI summarization with Llama-3.2-3B-Instruct
This demonstrates the enhanced quality with few-shot prompting
"""

import os
from dotenv import load_dotenv
from huggingface_hub import InferenceClient

load_dotenv()

def test_old_vs_new_model():
    """Compare BART (old) vs Llama (new) summarization quality."""
    
    hf_token = os.getenv('HUGGINGFACE_API_TOKEN')
    if not hf_token:
        print("âŒ HUGGINGFACE_API_TOKEN not found in .env file")
        return
    
    client = InferenceClient(api_key=hf_token)
    
    # Sample therapy report data
    sample_data = """
    Student: Alex Johnson
    Sessions: 8 sessions over 10 weeks
    Therapy Types: Speech Therapy, Occupational Therapy
    
    Session 1 (2024-01-15): Speech Therapy - Poor level
    Notes: Student struggles with pronunciation, very shy, limited vocabulary. Difficulty with 'th' and 's' sounds.
    
    Session 4 (2024-02-12): Speech Therapy - Average level
    Notes: Showing improvement in articulation. More willing to participate. Working on sentence formation.
    
    Session 8 (2024-03-22): Speech Therapy - Very Good level
    Notes: Excellent progress. Clear articulation, confident speaker, participates actively in discussions.
    """
    
    print("=" * 70)
    print("TESTING OLD vs NEW AI SUMMARIZATION")
    print("=" * 70)
    
    # Test 1: Old model (BART)
    print("\nðŸ“Š TEST 1: Old Model (facebook/bart-large-cnn)")
    print("-" * 70)
    try:
        old_result = client.summarization(
            sample_data,
            model="facebook/bart-large-cnn"
        )
        old_summary = old_result.get('summary_text', str(old_result)) if isinstance(old_result, dict) else str(old_result)
        print(f"Result:\n{old_summary}")
    except Exception as e:
        print(f"âŒ Error: {e}")
    
    # Test 2: New model (Llama) with few-shot prompting
    print("\n\nðŸš€ TEST 2: New Model (meta-llama/Llama-3.2-3B-Instruct)")
    print("-" * 70)
    
    # Few-shot prompt
    few_shot_prompt = """You are a professional therapy report analyst. Generate a brief, grammatically correct overview.

EXAMPLE:
Input: Student John, 5 sessions, Speech Therapy, progressed from Poor to Good
Output: John has demonstrated significant progress across five speech therapy sessions. Initially presenting with articulation difficulties and limited verbal participation, he has advanced to a Good performance level. His journey shows consistent improvement in communication skills, with notable gains in pronunciation clarity and conversational confidence.

NOW ANALYZE:
""" + sample_data + "\n\nGenerate professional overview:\n"
    
    try:
        # Use chat_completion for Llama models
        messages = [{"role": "user", "content": few_shot_prompt}]
        new_result = client.chat_completion(
            messages=messages,
            model="meta-llama/Llama-3.2-3B-Instruct",
            max_tokens=300,
            temperature=0.7
        )
        # Extract the response
        if hasattr(new_result, 'choices') and len(new_result.choices) > 0:
            new_summary = new_result.choices[0].message.content
        else:
            new_summary = str(new_result)
        print(f"Result:\n{new_summary}")
    except Exception as e:
        print(f"âŒ Error: {e}")
    
    print("\n" + "=" * 70)
    print("COMPARISON SUMMARY")
    print("=" * 70)
    print("Old Model (BART):")
    print("  âŒ Often grammatically incorrect")
    print("  âŒ Just concatenates phrases")
    print("  âŒ No professional structure")
    print()
    print("New Model (Llama-3.2 with Few-Shot):")
    print("  âœ… Grammatically correct sentences")
    print("  âœ… Professional clinical language")
    print("  âœ… Structured, coherent analysis")
    print("  âœ… Contextually relevant details")
    print("=" * 70)


def test_all_analysis_sections():
    """Test all analysis sections with the new approach."""
    
    print("\n\n" + "=" * 70)
    print("TESTING ALL ANALYSIS SECTIONS")
    print("=" * 70)
    
    hf_token = os.getenv('HUGGINGFACE_API_TOKEN')
    if not hf_token:
        print("âŒ HUGGINGFACE_API_TOKEN not found")
        return
    
    client = InferenceClient(api_key=hf_token)
    
    # Test Baseline Analysis
    print("\nðŸ“‹ BASELINE ANALYSIS (Initial Assessment)")
    print("-" * 70)
    
    baseline_prompt = """You are a professional therapy assessment specialist.

EXAMPLE:
Input: Maria, early sessions: poor articulation, shy, limited vocabulary, difficulty with 's' and 'th' sounds
Output: Maria's initial speech therapy sessions identified multiple areas requiring intervention. She exhibited poor articulation, particularly with 's' and 'th' phonemes, accompanied by limited expressive vocabulary and significant shyness affecting verbal participation. The baseline assessment established her starting point for a comprehensive speech development program.

NOW ANALYZE:
Student: Alex Johnson
Session 1: Poor level, struggles with pronunciation, very shy, limited vocabulary
Session 2: Poor level, difficulty with 'th' sounds, reluctant to participate

Generate professional baseline analysis:
"""
    
    try:
        messages = [{"role": "user", "content": baseline_prompt}]
        result = client.chat_completion(
            messages=messages,
            model="meta-llama/Llama-3.2-3B-Instruct",
            max_tokens=250,
            temperature=0.7
        )
        print(_extract_text(result))
    except Exception as e:
        print(f"âŒ Error: {e}")
    
    # Test Current Status
    print("\n\nðŸ“Š CURRENT STATUS ANALYSIS")
    print("-" * 70)
    
    current_prompt = """You are a professional therapy progress analyst.

EXAMPLE:
Input: Michael, recent sessions, Excellent level, confident speaker, 95% articulation accuracy
Output: Michael currently demonstrates excellent speech and communication abilities. Recent sessions document his confident participation in classroom discussions, achieving 95% articulation accuracy across all targeted phonemes. He now leads group conversations independently and exhibits natural, fluent speech patterns appropriate for his age group.

NOW ANALYZE:
Student: Alex Johnson
Recent Session 7: Very Good level, clear articulation, confident speaker
Recent Session 8: Very Good level, participates actively, leads discussions

Generate professional current status:
"""
    
    try:
        messages = [{"role": "user", "content": current_prompt}]
        result = client.chat_completion(
            messages=messages,
            model="meta-llama/Llama-3.2-3B-Instruct",
            max_tokens=250,
            temperature=0.7
        )
        print(_extract_text(result))
    except Exception as e:
        print(f"âŒ Error: {e}")
    
    # Test Recommendations
    print("\n\nðŸ’¡ RECOMMENDATIONS")
    print("-" * 70)
    
    recommendations_prompt = """You are a professional therapy coordinator.

EXAMPLE:
Input: Student progressed from Poor to Excellent, all goals met
Output: Based on the exceptional progress demonstrated, the following recommendations are proposed: (1) Transition to mainstream classroom activities without direct therapeutic support, (2) Implement monthly monitoring through teacher observations, (3) Schedule a follow-up assessment in six months to confirm sustained progress. The student has successfully met all initial treatment objectives.

NOW GENERATE FOR:
Student: Alex Johnson
Progress: Poor â†’ Very Good over 8 sessions
Current Status: Confident speaker, active participant

Generate professional recommendations:
"""
    
    try:
        messages = [{"role": "user", "content": recommendations_prompt}]
        result = client.chat_completion(
            messages=messages,
            model="meta-llama/Llama-3.2-3B-Instruct",
            max_tokens=300,
            temperature=0.7
        )
        print(_extract_text(result))
    except Exception as e:
        print(f"âŒ Error: {e}")
    
    print("\n" + "=" * 70)
    print("âœ… ALL TESTS COMPLETE")
    print("=" * 70)


def _extract_text(result):
    """Extract text from Hugging Face result."""
    if isinstance(result, str):
        return result.strip()
    elif hasattr(result, 'choices') and len(result.choices) > 0:
        return result.choices[0].message.content.strip()
    elif isinstance(result, dict):
        return result.get('generated_text', str(result)).strip()
    return str(result)


if __name__ == "__main__":
    print("ðŸ§ª IMPROVED AI SUMMARIZATION TEST")
    print("Testing Llama-3.2-3B-Instruct with Few-Shot Prompting")
    print()
    
    test_old_vs_new_model()
    test_all_analysis_sections()
    
    print("\n\nâœ¨ NEXT STEPS:")
    print("1. The backend is now using Llama-3.2-3B-Instruct")
    print("2. Few-shot examples ensure proper grammar and structure")
    print("3. Start the backend: cd backend && uvicorn app.main:app --reload")
    print("4. Test the endpoint: POST /api/v1/therapy-reports/summary/ai")
    print("5. The frontend will automatically use the improved AI!")
